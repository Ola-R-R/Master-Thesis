
\section{Discussion}

\noindent The results in Table \ref{table:pca results period 3} shows that the first component already explains almost $92\%$ of the variation by itself. One could therefore argue that using one component is more than sufficient for my implementation. We can see from the results for the whole dataset in Table \ref{table:pca results} that the first component only explains almost $88\%$ of the variation. If I had used the whole dataset I would therefore be more sure that two components are necessary because the first component explains less than the first component for the data from period 3. One principal component only allows for one source of randomness in my model. Choosing more components will allow for more subtle movements in the term structure of interest rates.

Figure \ref{fig:scree plot period 3} shows an "elbow" at the second component, or a drop of. According to \cite[p.~409]{intro_stat_learning} this is therefore the ideal number of components. Choosing three components leads to explaining $98.9\%$ of the variation, which would lead to overfitting. This would make my model good at predicting the data I already have, but it would not create good realizations into the future.

The observed volatilities from the whole dataset in Figure \ref{fig:observed volatilites} and period $3$ in Figure \ref{fig:observed volatilites period 3} seem reasonable. As expected, the observed volatilities from period $3$ are larger than the whole dataset, with lower volatilities at short tenors. It was therefore necessary to only use period $3$ because the rest of the data is not relevant. This is because the whole dataset includes periods with less volatility. If I had used the whole dataset instead of only period 3, I would get simulations that fluctuate less compared to the results I have now. This would potentially generate the wrong fair values of IRDs. From Figure \ref{fig:corr plot}, we could already see that there were two different movements in the data. The PCA correctly extracted this \newpage \noindent because the $0.5$-, $0.75$-, $1$-year interest rates have a completely different structure compared to the rest of the rates, which have the same constant movement for the volatility for the first component. The volatility for the second volatility adds a little more deviations to the larger tenors.

The fitted volatilities from period $3$ in Figure \ref{fig:fitted volatilites period 3} show that the volatilities fitted using spline regression fit the volatility for the first factor much better than the volatilities fitted using polynomial regression. The fitted volatilities for the second factor however are identical. The spline models therefore seem superior to the polynomial models because they fit the volatility structure better. Spline regression is, however, much harder to fit to data without any existing programming packages. The polynomial model could therefore be a good substitute because it is much simpler to fit.

The drift in the spline models are therefore also superior because the volatilities are better fit. The curve in the polynomial model drifts may give undesirable results in the simulations.

The residual vs. fits plots in Figures \ref{fig:resid vs fit p 1} and \ref{fig:resid vs fit p 2} shows that the homoscedastic errors assumption maybe was wrong for the models using procedure $2$, but correct for the models using procedure $1$. When using procedure $2$ there are a few fitted values slightly larger than the others, which does not exhibit the same spread as the others. These can be outliers because there are so few points. Therefore I conclude that the homoscedastic errors assumption is correct for all models.

The Q-Q plots in Figures \ref{fig:qq p 1} and \ref{fig:qq p 2} shows that the normality assumption holds for every tenors when using procedure $2$, but when using procedure $1$ there are some tenors where the real data has heavier tails than the model can predict. From my experience from working with real data I know that this is normal. Therefore I conclude that the normality assumption is correct for all models.

We see that procedure 1 has a higher error at the shortest tenor than procedure 2 from looking at Figure \ref{fig:Error}. This means that procedure 2 generates better realizations for the smallest tenors, but at slightly larger tenors, procedure 1 generates better realizations. At the largest tenors they generate almost identical errors. This is true for both the MAPEs and the MRSPEs calculated. From this I will say that procedure 2 is better because I intend to use the shortest maturities to discount my IRD prices. The errors are not that different however, so any model would give sufficient predictions.

Using procedure 1, I can generate $10,000$ realizations $10$ years into the future in approximately $4,000$ seconds. Procedure $2$ however, can generate the same number of realizations in less than $300$ seconds. This makes procedure $2$ more than $13$ times faster than procedure $1$, and therefore procedure $2$ is the more efficient method. Procedure $1$ \newpage \noindent also generates many discontinuities due to the NS model. This is because when the yield curve changes slightly, the NS model can give a very different curve. We could already see in Figure \ref{fig:current zero-coupon yield curve} that the NS model deviates slightly from the real data. Because the spline model predicts that the $10$-year rates begin to flatten earlier than what the polynomial model predicts, the realizations from Procedure $1$ are significantly narrower toward the end for the spline model. The spline model also generally predicts slightly lower rates for the $10$-year interest rate, and this is because the drift is slightly lower for this rate when compared to the polynomial model.

There is not a big difference between the prices generated by each model. The models using procedure 2 are very similar, meaning that the predictions are stable. The prices generated using procedure 1 deviate slightly. The prices over time always has positive expected value, which means that it would make sense to agree to this exact contract. Due to the prices from procedure $2$ being almost identical, I conclude that both the polynomial and spline models works equally as good. The choice of model is irrelevant because they predict the same fair value.

There is a very high expected exposure during the lifetime of the contract. Each model generate an expected exposure of $100,000$ during the first $5$ years. This means that the bondholder stand to lose about $100,000$ if the other party fails to pay their share. The exposure thereafter decreases slowly to zero. The choice of model is irrelevant because they predict the same exposure during the lifetime of the contract.



\section{Conclusion}

\noindent In conclusion, I successfully implemented the multi-factor HJM model. My different procedures generated similar interest rate swap prices, but procedure 2 generated realizations faster than procedure 1 and the realizations were more stable. Every model met my assumptions. I conclude that using the spline model and procedure $2$ is the superior method for generating interest rate realizations into the future due to the speed and stability of procedure $2$ and the fact that the fitted volatilities using the spline regression model fit the volatility structure the best.

One could have argued that a one-factor HJM model would be sufficient for the data because in the PCA result we could see that one principal component already explains more than $90\%$ of the variation in the dataset. This would speed up simulations and calibration, but this would mean that the dynamic in the term structure of interest rates are driven by a single source of randomness. Two sources of randomness allow for more subtle movements.

One could also argue that the polynomial HJM model is adequate, because the \newpage \noindent simulations generated using procedure $2$ are similar to those generated by the spline model. The polynomial model is also far easier to fit, allowing for simple implementations. However, given that there are already made packages in R that helps with spline regression, I prefer the spline model and mean that it is the superior model choice.


\section{Future work}

\noindent Possible directions for future work or improvements of the model could be to get more historical data. This would allow me to investigate how long different periods lasts, which could give me an indicator of how long my realizations are expected to be reliable.

I could also try to calibrate the model to real world exotic IRD prices instead of using PCA to fit the volatility structure. This would allow for more accurate pricing of these kinds of exotic IRDs. I could then compare these prices with the prices I can simulate from the model used in this thesis.

Another direction could be to train the model when accounting for more than one day difference. For example holidays or weekends. I could then account for these days when predicting into the future.

